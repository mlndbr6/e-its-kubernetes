grafana:
  rbac:
    create: true
    pspEnabled: false
    pspUseAppArmor: false
    namespaced: false
    extraRoleRules: []

  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000

  livenessProbe:
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 60
    timeoutSeconds: 30
    failureThreshold: 10

  testFramework:
    enabled: true
    ## The type of Helm hook used to run this test. Defaults to test.
    ## ref: https://helm.sh/docs/topics/charts_hooks/#the-available-hooks
    ##
    # hookType: test
    image:
      # -- The Docker registry
      registry: docker.io
      repository: bats/bats
      tag: "v1.4.1"
    imagePullPolicy: IfNotPresent
    securityContext: {}
    containerSecurityContext: {}
    resources: {}
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi


  securityContext:
    runAsNonRoot: true
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472

  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault

  downloadDashboardsImage:
    # -- The Docker registry
    registry: docker.io
    repository: curlimages/curl
    tag: 8.9.1
    sha: ""
    pullPolicy: IfNotPresent

  downloadDashboards:
    env: {}
    envFromSecret: ""
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    envValueFrom: {}
    #  ENV_NAME:
    #    configMapKeyRef:
    #      name: configmap-name
    #      key: value_key

  service:
    enabled: true
  serviceMonitor:
    ## If true, a ServiceMonitor CR is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    path: /metrics
    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)
    labels: {}
    interval: 30s
    scheme: http
    tlsConfig: {}
    scrapeTimeout: 30s
    relabelings: []
    metricRelabelings: []
    basicAuth: {}
    targetLabels: []

  ingress:
    enabled: true
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # Values can be templated
    annotations:
      prometheus.io/probe: "false"
      external-dns.alpha.kubernetes.io/ttl: "60"
      kubernetes.io/ingress.class: "haproxy"
      cert-manager.io/cluster-issuer: "azure-dns"
    ingressClassName: "haproxy"
    path: /
    pathType: Prefix
    hosts:
      - grafana.lndbrg.tech
    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation
    ## Or for k8s > 1.19
    # - path: /*
    #   pathType: Prefix
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: use-annotation
    tls:
     - secretName: grafana-server-tls
       hosts:
         - grafana.lndbrg.tech


  resources:
   limits:
     cpu: 300m
     memory: 1Gi
   requests:
     cpu: 100m
     memory: 128Mi


  tolerations:
    - key: "monitoring"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "kubernetes.io/hostname"
            operator: In
            values:
            - capi-ha-monitoring000000
            - capi-ha-monitoring000001

  persistence:
    type: pvc
    enabled: true
    storageClassName: standard
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
    # selectorLabels: {}
    ## Sub-directory of the PV to mount. Can be templated.
    # subPath: ""
    ## Name of an existing PVC. Can be templated.
    # existingClaim:
    ## Extra labels to apply to a PVC.
    extraPvcLabels: {}
    disableWarning: false

    ## If persistence is not enabled, this allows to mount the
    ## local storage in-memory to improve performance
    ##
    inMemory:
      enabled: false
      ## The maximum usage on memory medium EmptyDir would be
      ## the minimum value between the SizeLimit specified
      ## here and the sum of memory limits of all containers in a pod
      ##
      # sizeLimit: 300Mi

    ## If 'lookupVolumeName' is set to true, Helm will attempt to retrieve
    ## the current value of 'spec.volumeName' and incorporate it into the template.
    lookupVolumeName: true

  initChownData:
    ## If false, data ownership will not be reset at startup
    ## This allows the grafana-server to be run with an arbitrary user
    ##
    enabled: true

    ## initChownData container image
    ##
    image:
      # -- The Docker registry
      registry: docker.io
      repository: library/busybox
      tag: "1.31.1"
      sha: ""
      pullPolicy: IfNotPresent

    ## initChownData resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
     limits:
       cpu: 300m
       memory: 512Mi
     requests:
       cpu: 10m
       memory: 100Mi
    securityContext:
      readOnlyRootFilesystem: true
      runAsNonRoot: false
      runAsUser: 0
      seccompProfile:
        type: RuntimeDefault
      capabilities:
        add:
          - CHOWN
        drop:
          - ALL

  adminUser: admin
  envValueFrom: {}
  envFromSecret: ""

  enableServiceLinks: true
  plugins: []
  datasources:
   datasources.yaml:
     apiVersion: 1
     datasources:
     - name: Prometheus
       type: prometheus
       url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack.svc.cluster.local:9090
       access: proxy
       isDefault: true
     - name: Loki
       type: loki
       access: proxy
       url: http://loki.loki.svc.cluster.local:3100
       jsonData:
         timeout: 60
         maxLines: 1000
  #    - name: Prometheus

  alerting: {}
    # policies.yaml:
    #   apiVersion: 1
    #   policies:
    #     - orgId: 1
    #       receiver: first_uid
    #
    # rules.yaml:
    #   apiVersion: 1
    #   groups:
    #     - orgId: 1
    #       name: '{{ .Chart.Name }}_my_rule_group'
    #       folder: my_first_folder
    #       interval: 60s
    #       rules:
    #         - uid: my_id_1
    #           title: my_first_rule
    #           condition: A
    #           data:
    #             - refId: A
    #               datasourceUid: '-100'
    #               model:
    #                 conditions:
    #                   - evaluator:
    #                       params:
    #                         - 3
    #                       type: gt
    #                     operator:
    #                       type: and
    #                     query:
    #                       params:
    #                         - A
    #                     reducer:
    #                       type: last
    #                     type: query
    #                 datasource:
    #                   type: __expr__
    #                   uid: '-100'
    #                 expression: 1==0
    #                 intervalMs: 1000
    #                 maxDataPoints: 43200
    #                 refId: A
    #                 type: math
    #           dashboardUid: my_dashboard
    #           panelId: 123
    #           noDataState: Alerting
    #           for: 60s
    #           annotations:
    #             some_key: some_value
    #           labels:
    #             team: sre_team_1
    #
    # contactpoints.yaml:
    #   secret:
    #     apiVersion: 1
    #     contactPoints:
    #       - orgId: 1
    #         name: cp_1
    #         receivers:
    #           - uid: first_uid
    #             type: pagerduty
    #             settings:
    #               integrationKey: XXX
    #               severity: critical
    #               class: ping failure
    #               component: Grafana
    #               group: app-stack
    #               summary: |
    #                 {{ `{{ include "default.message" . }}` }}
    #
    # templates.yaml:
    #   apiVersion: 1
    #   templates:
    #     - orgId: 1
    #       name: my_first_template
    #       template: |
    #         {{ `
    #         {{ define "my_first_template" }}
    #         Custom notification message
    #         {{ end }}
    #         ` }}
    #
    # mutetimes.yaml
    #   apiVersion: 1
    #   muteTimes:
    #     - orgId: 1
    #       name: mti_1
    #       # refer to https://prometheus.io/docs/alerting/latest/configuration/#time_interval-0
    #       time_intervals: {}

  ## Configure notifiers
  ## ref: http://docs.grafana.org/administration/provisioning/#alert-notification-channels
  ##
  notifiers: {}
  #  notifiers.yaml:
  #    notifiers:
  #    - name: email-notifier
  #      type: email
  #      uid: email1
  #      # either:
  #      org_id: 1
  #      # or
  #      org_name: Main Org.
  #      is_default: true
  #      settings:
  #        addresses: an_email_address@example.com
  #    delete_notifiers:

  ## Configure grafana dashboard providers
  ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
  ##
  ## `path` must be /var/lib/grafana/dashboards/<provider_name>
  ##
  dashboardProviders: {}
  #  dashboardproviders.yaml:
  #    apiVersion: 1
  #    providers:
  #    - name: 'default'
  #      orgId: 1
  #      folder: ''
  #      type: file
  #      disableDeletion: false
  #      editable: true
  #      options:
  #        path: /var/lib/grafana/dashboards/default

  ## Configure grafana dashboard to import
  ## NOTE: To use dashboards you must also enable/configure dashboardProviders
  ## ref: https://grafana.com/dashboards
  ##
  ## dashboards per provider, use provider name as key.
  ##
  dashboards: {}
    # default:
    #   some-dashboard:
    #     json: |
    #       $RAW_JSON
    #   custom-dashboard:
    #     file: dashboards/custom-dashboard.json
    #   prometheus-stats:
    #     gnetId: 2
    #     revision: 2
    #     datasource: Prometheus
    #   local-dashboard:
    #     url: https://example.com/repository/test.json
    #     token: ''
    #   local-dashboard-base64:
    #     url: https://example.com/repository/test-b64.json
    #     token: ''
    #     b64content: true
    #   local-dashboard-gitlab:
    #     url: https://example.com/repository/test-gitlab.json
    #     gitlabToken: ''
    #   local-dashboard-bitbucket:
    #     url: https://example.com/repository/test-bitbucket.json
    #     bearerToken: ''
    #   local-dashboard-azure:
    #     url: https://example.com/repository/test-azure.json
    #     basic: ''
    #     acceptHeader: '*/*'

  ## Reference to external ConfigMap per provider. Use provider name as key and ConfigMap name as value.
  ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both.
  ## ConfigMap data example:
  ##
  ## data:
  ##   example-dashboard.json: |
  ##     RAW_JSON
  ##
  dashboardsConfigMaps: {}
  #  default: ""

  ## Grafana's primary configuration
  ## NOTE: values in map will be converted to ini format
  ## ref: http://docs.grafana.org/installation/configuration/
  ##
  grafana.ini:
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    grafana_net:
      url: https://grafana.net
    server:
      domain: "{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ tpl (.Values.ingress.hosts | first) . }}{{ else }}''{{ end }}"